{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8gU7AYPXMmA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. Make sure you fill in any place that says `# BEGIN CODE HERE #END CODE HERE`. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run\" (denoted by a play symbol). Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). \n",
    "\n",
    " **What you need to remember:**\n",
    "\n",
    "- Run your cells using SHIFT+ENTER (or \"Run cell\")\n",
    "- Write code in the designated areas using Python 3 only\n",
    "- Do not modify the code outside of the designated areas\n",
    "- In some cases you will also need to explain the results. There will also be designated areas for that. \n",
    "\n",
    "Fill in your **NAME** and **AEM** below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lO-jJrtNXMmH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NAME = "Dimitrios Sourlantzis""
    "COURSE = "Machine Learning - AUTH""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh0EE7BJXMmJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_VpnGyWXMmK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 3 - Ensemble Methods #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dQ9XoGQXMmK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Welcome to your third assignment. This exercise will test your understanding on Ensemble Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JvHYIhS-XMmL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Always run this cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, make_regression, load_digits, fetch_california_housing, make_hastie_10_2\n",
    "from sklearn.ensemble import BaggingClassifier, StackingClassifier, GradientBoostingClassifier, VotingClassifier, RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split,  RepeatedStratifiedKFold, cross_validate, cross_val_score, KFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, make_scorer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from numpy import round, mean\n",
    "import xgboost as xgb\n",
    "# USE THE FOLLOWING RANDOM STATE FOR YOUR CODE\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joKwpih2XMmM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download the Dataset ##\n",
    "Download the dataset using the following cell or from this [link](https://github.com/sakrifor/public/tree/master/machine_learning_course/EnsembleDataset) and put the files in the same folder as the .ipynb file. \n",
    "In this assignment you are going to work with a dataset originated from the [ImageCLEFmed: The Medical Task 2016](https://www.imageclef.org/2016/medical) and the **Compound figure detection** subtask. The goal of this subtask is to identify whether a figure is a compound figure (one image consists of more than one figure) or not. The train dataset consits of 4197 examples/figures and each figure has 4096 features which were extracted using a deep neural network. The *CLASS* column represents the class of each example where 1 is a compoung figure and 0 is not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJdwPr0bXMmM",
    "outputId": "48e91e53-d402-42ba-9e95-c95966852a38",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('test_set_noclass.csv', <http.client.HTTPMessage at 0x7fbf19882150>)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import urllib.request\n",
    "url_train = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/train_set.csv'\n",
    "filename_train = 'train_set.csv'\n",
    "urllib.request.urlretrieve(url_train, filename_train)\n",
    "url_test = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/test_set_noclass.csv'\n",
    "filename_test = 'test_set_noclass.csv'\n",
    "urllib.request.urlretrieve(url_test, filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "t0OVtYr7XMmN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load the data\n",
    "train_set = pd.read_csv(\"train_set.csv\").sample(frac=1).reset_index(drop=True)\n",
    "train_set.head()\n",
    "X = train_set.drop(columns=['CLASS'])\n",
    "y = train_set['CLASS'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxOGHSmqXMmO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.0 Testing different ensemble methods ##\n",
    "In this part of the assignment you are asked to create and test different ensemble methods using the train_set.csv dataset. You should use **10-fold cross validation** for your tests and report the average f-measure weighted and balanced accuracy of your models. You can use [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) and select both metrics to be measured during the evaluation. Otherwise, you can use [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold).\n",
    "\n",
    "### !!! Use n_jobs=-1 where is posibble to use all the cores of a machine for running your tests ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww_u4OlrXMmO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Voting ###\n",
    "Create a voting classifier which uses three **simple** estimators/classifiers. Test both soft and hard voting and choose the best one. Consider as simple estimators the following:\n",
    "\n",
    "\n",
    "*   Decision Trees\n",
    "*   Linear Models\n",
    "*   Probabilistic Models (Naive Bayes)\n",
    "*   KNN Models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwvPacgkXMmP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "\n",
    "cls1 = ('decision_tree_vanilla', DecisionTreeClassifier(random_state = RANDOM_STATE))# Classifier #1 \n",
    "cls2 = ('KNeighbors', KNeighborsClassifier())# Classifier #2\n",
    "cls3 = ('logistic classifier',LogisticRegression(random_state = RANDOM_STATE)) # Classifier #3\n",
    "\n",
    "model = [cls1, cls2, cls3]\n",
    "\n",
    "soft_vcls = VotingClassifier(estimators=model, voting='soft', n_jobs=-1) # Voting Classifier\n",
    "hard_vcls = VotingClassifier(estimators=model, voting='hard', n_jobs=-1) # Voting Classifier\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),'f1': make_scorer(f1_score)}\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "svcls_scores = cross_validate(soft_vcls, X, y, scoring=scoring, cv=kfold, return_train_score = True, n_jobs=1, error_score='raise')\n",
    "\n",
    "s_avg_fmeasure = mean(svcls_scores[\"test_f1\"]) # The average f-measure\n",
    "s_avg_accuracy = mean(svcls_scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "hvcls_scores = cross_validate(hard_vcls, X, y, scoring=scoring, cv=kfold, return_train_score = True, n_jobs=1, error_score='raise')\n",
    "\n",
    "h_avg_fmeasure = mean(hvcls_scores[\"test_f1\"]) # The average f-measure\n",
    "h_avg_accuracy = mean(hvcls_scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQQvClrmXMmQ",
    "outputId": "209760a6-b8e3-49b4-ecf5-858a5e823eae",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifier:\n",
      "VotingClassifier(estimators=[('decision_tree_vanilla',\n",
      "                              DecisionTreeClassifier(random_state=42)),\n",
      "                             ('KNeighbors', KNeighborsClassifier()),\n",
      "                             ('logistic classifier',\n",
      "                              LogisticRegression(random_state=42))],\n",
      "                 n_jobs=-1, voting='soft')\n",
      "F1 Weighted-Score: 0.8541 & Balanced Accuracy: 0.8274\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(soft_vcls)\n",
    "print(\"F1 Weighted-Score: {} & Balanced Accuracy: {}\".format(round(s_avg_fmeasure,4), round(s_avg_accuracy,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-iJK9pFaDka",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should achive above 82% (Soft Voting Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRNkVAvEYVbn",
    "outputId": "8382cac5-9884-4628-8990-af30cb36bc38",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifier:\n",
      "VotingClassifier(estimators=[('decision_tree_vanilla',\n",
      "                              DecisionTreeClassifier(random_state=42)),\n",
      "                             ('KNeighbors', KNeighborsClassifier()),\n",
      "                             ('logistic classifier',\n",
      "                              LogisticRegression(random_state=42))],\n",
      "                 n_jobs=-1)\n",
      "F1 Weighted-Score: 0.8527 & Balanced Accuracy: 0.8263\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(hard_vcls)\n",
    "print(\"F1 Weighted-Score: {} & Balanced Accuracy: {}\".format(round(h_avg_fmeasure,4), round(h_avg_accuracy,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6M0CZO6aEHi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should achieve above 80% in both! (Hard Voting Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVPuIxwFXMmR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Stacking ###\n",
    "Create a stacking classifier which uses two more complex estimators. Try different simple classifiers (like the ones mentioned before) for the combination of the initial estimators. Report your results in the following cell.\n",
    "\n",
    "Consider as complex estimators the following:\n",
    "\n",
    "*   Random Forest\n",
    "*   SVM\n",
    "*   Gradient Boosting\n",
    "*   MLP\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HX6T1qrFXMmS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "\n",
    "cls1 = RandomForestClassifier(random_state = RANDOM_STATE, n_jobs=-1) # Classifier #1 \n",
    "cls2 = GradientBoostingClassifier(random_state = RANDOM_STATE) # Classifier #2 \n",
    "cls3 = MLPClassifier(random_state = RANDOM_STATE) # Classifier #3 (Optional)\n",
    "model = [('random forest', cls1), ('gradient_boosting', cls2), ('MLP', cls3)]\n",
    "scls = StackingClassifier(estimators = model, final_estimator = LogisticRegression(), n_jobs=-1) # Stacking Classifier\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),'f1': make_scorer(f1_score)}\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "scores = cross_validate(scls, X, y, scoring=scoring, cv=kfold, return_train_score = True, n_jobs=1, error_score='raise')\n",
    "avg_fmeasure = mean(scores['test_f1']) # The average f-measure\n",
    "avg_accuracy = mean(scores['test_accuracy']) # The average accuracy\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JLRzkQ1XMmT",
    "outputId": "410f7e0f-be3a-4691-efef-aeb4121f549f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifier:\n",
      "StackingClassifier(estimators=[('random forest',\n",
      "                                RandomForestClassifier(n_jobs=-1,\n",
      "                                                       random_state=42)),\n",
      "                               ('gradient_boosting',\n",
      "                                GradientBoostingClassifier(random_state=42)),\n",
      "                               ('MLP', MLPClassifier(random_state=42))],\n",
      "                   final_estimator=LogisticRegression(), n_jobs=-1)\n",
      "F1 Weighted Score: 0.8815 & Balanced Accuracy: 0.8594\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(scls)\n",
    "print(\"F1 Weighted Score: {} & Balanced Accuracy: {}\".format(round(avg_fmeasure,4), round(avg_accuracy,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcgOx-HPvBI-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should achieve above 85% in both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-nqW51xXMmU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.0 Randomization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPG8MdFLXMmV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**2.1** You are asked to create three ensembles of decision trees where each one uses a different method for producing homogeneous ensembles. Compare them with a simple decision tree classifier and report your results in the dictionaries (dict) below using as key the given name of your classifier and as value the f1_weighted/balanced_accuracy score. The dictionaries should contain four different elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmkaP-DjXMmV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "ens1 = ExtraTreesClassifier(n_jobs = -1, random_state = RANDOM_STATE)\n",
    "ens2 = GradientBoostingClassifier(random_state = RANDOM_STATE)\n",
    "ens3 = RandomForestClassifier(n_jobs = -1, random_state = RANDOM_STATE)\n",
    "tree = DecisionTreeClassifier(random_state = RANDOM_STATE)\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),'f1': make_scorer(f1_score)}\n",
    "\n",
    "f_measures = dict()\n",
    "accuracies = dict()\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "extra_tree_score =  cross_validate(ens1, X,y, scoring = scoring, cv = kfold , n_jobs= -1)\n",
    "f_measures[\"Extra tree\"] = mean(extra_tree_score['test_f1'])\n",
    "accuracies[\"Extra tree\"] = mean(extra_tree_score['test_accuracy'])\n",
    "\n",
    "gradient_boosting_score =  cross_validate(ens2, X,y, scoring = scoring, cv = kfold , n_jobs= -1)\n",
    "f_measures[\"Gradient Boosting\"] = mean(gradient_boosting_score['test_f1'])\n",
    "accuracies[\"Gradient Boosting\"] = mean(gradient_boosting_score['test_accuracy'])\n",
    "\n",
    "random_forest_score =  cross_validate(ens3, X,y, scoring = scoring, cv = kfold , n_jobs= -1)\n",
    "f_measures[\"Random Forest\"] = mean(extra_tree_score['test_f1'])\n",
    "accuracies[\"Random Forest\"] = mean(extra_tree_score['test_accuracy'])\n",
    "\n",
    "tree_score =  cross_validate(tree, X,y, scoring = scoring, cv = kfold , n_jobs= -1)\n",
    "f_measures[\"Decision Tree\"] = mean(tree_score['test_f1'])\n",
    "accuracies[\"Decision Tree\"] = mean(tree_score['test_accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUqhDUuCXMmW",
    "outputId": "d1315c14-b382-4bfd-97e7-e75703cb8fc3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "RandomForestClassifier(n_jobs=-1, random_state=42)\n",
      "DecisionTreeClassifier(random_state=42)\n",
      "Classifier:Extra tree -  F1 Weighted:0.8485\n",
      "Classifier:Gradient Boosting -  F1 Weighted:0.8519\n",
      "Classifier:Random Forest -  F1 Weighted:0.8485\n",
      "Classifier:Decision Tree -  F1 Weighted:0.7471\n",
      "Classifier:Extra tree -  BalancedAccuracy:0.8124\n",
      "Classifier:Gradient Boosting -  BalancedAccuracy:0.8196\n",
      "Classifier:Random Forest -  BalancedAccuracy:0.8124\n",
      "Classifier:Decision Tree -  BalancedAccuracy:0.7057\n"
     ]
    }
   ],
   "source": [
    "print(ens1)\n",
    "print(ens2)\n",
    "print(ens3)\n",
    "print(tree)\n",
    "for name,score in f_measures.items():\n",
    "    print(\"Classifier:{} -  F1 Weighted:{}\".format(name,round(score,4)))\n",
    "for name,score in accuracies.items():\n",
    "    print(\"Classifier:{} -  BalancedAccuracy:{}\".format(name,round(score,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqdXTE_2XMmX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**2.2** Describe your classifiers and your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rU9POFftXMmX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "# ANSWER\n",
    "In this block of code we used the 3 following ensembles:\n",
    "\n",
    "\n",
    "*   **Extra Trees Classifier**: This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n",
    "\n",
    "*   **Gradient Boosting Classifier**: Builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage `n_classes_` regression trees are fit on the negative gradient of the loss function, e.g. binary or multiclass log loss. Binary classification is a special case where only a single regression tree is induced.\n",
    "\n",
    "*  **Random Forests Classifier**: A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree.\n",
    "\n",
    "\n",
    "(More info can be found in Sklearn documentation or TowardsDataScience [website](https://towardsdatascience.com/)) \n",
    "\n",
    "These 3 models have the same hyperparameters as a Decision Tree Model (`Criterion`, `max_depth`,` min_samples_per_leaf` etc.). An extra but important parameter is `n_estiomators` and it is the number of estimators used in a model (number of trees). This last parameter can play a huge role on accuracy (reduce underfitting/overfitting etc) and training (more on ***2.3***). \n",
    "\n",
    "All classifier run in their default configuration, without any hyperparameter tuning or feature engineering.\n",
    "\n",
    "All the classifiers use bagging or boosting techniques, thus they are homogeneous\n",
    "\n",
    "Let's compare the ensembles with a normal decision tree. These three ensembles performed relatively well, scoring 84% accuracy and above 80% F1 score or slightly above (Best one was GradientBoosting, then RandomForests then ExtraTrees both on accuracy and F1). \n",
    "\n",
    "The normal tree classifier, however performed worse, scoring a mediocre 74.1% accuracy and below 70% F1 Score. This is a sign that the normal vanilla decision tree suffered from overfitting while the ensembles avoided this obstacle. The 3 ensembles are capable of controlling over-fitting, something a normal vanilla decision tree cannot, thus requiring further configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkJeuV1FXMmX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**2.3** Increasing the number of estimators in a bagging classifier can drastically increase the training time of a classifier. Is there any solution to this problem? Can the same solution be applied to boosting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApNEPcWEXMmY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "# ANSWER\n",
    "The most important parameter for bagged decision trees is the number of trees (`n_estimators`). Ideally, this should be increased until no further improvement is seen in the model.An optimal number of trees should be found in a logarithmic scale (1, 10, 100, 1000, ...). With this solution, we have achieve high perfomance with normal training time.\n",
    "\n",
    "Can we apply the same to boosting classifiers? In order to answer that question we need to examine each ensemble on how it operates:\n",
    "\n",
    "* **Bagging** is a parallel method that fits different, considered learners independently from each other, making it possible to train them simultaneously. Then, every output from the learner is combined to make more accurate predictions \n",
    "\n",
    "* **Boosting** is a sequential ensemble method that iteratively adjusts the weight of observation as per the last classification. Trees are added one at a time, and existing trees in the model are not changed. The objective here is to minimize a loss function. In order to minimize loss, when a new tree is added it is configured in a way that minimizes further the residual loss.\n",
    "\n",
    "Bagging can run on parallel but it is still very possible to drastically increase the training time of a classifier thus it benefits from finding an optimal number of estimators\n",
    "\n",
    "Boosting cannot run in paraller since it requires the models previous state to calculate the current state. Having a large number of estimators in Boosting can drastically, and with a higher rate than Bagging, increase the training time of a classifier. \n",
    "\n",
    "This means that finding an optimal number of estimators for Boosting is very important and the solution in the first paragraph can really help a Boosting classifier reduce its training time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgvsCbUGXMmY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.0 Creating the best classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6daX2mRXMmZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**3.1** In this part of the assignment you are asked to train the best possible ensemble! Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure (weighted) & balanced accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code. Can you achieve a balanced accuracy over 83-84%?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "ozS0AHLivPSQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "This cell is for hyperparameter tuning. Due to long runtime, optimal hyperparameters have been assigned to each model from previous session.\n",
    "RUNTIME: 2+ Hours!\n",
    "To run, chage False to True\n",
    "\"\"\"\n",
    "\n",
    "if False:\n",
    "\n",
    "  kfold = KFold(n_splits=10, random_state=RANDOM_STATE, shuffle=True) #K-folding\n",
    "\n",
    "  param_grid_tree = {\n",
    "      'criterion' : ['gini', 'entropy'],\n",
    "      'bootstrap': [True, False],\n",
    "      'max_depth': [5, 10, 15],\n",
    "      'max_features': ['auto', 'sqrt'],\n",
    "      'max_leaf_nodes': [12, 16, 18],\n",
    "      'min_samples_leaf': [2, 5, 8],\n",
    "      'n_estimators': [5,50,250,500]\n",
    "  }\n",
    "\n",
    "  param_grid_MLP = {\n",
    "      'activation': ['logistic', 'tanh', 'relu'],\n",
    "      'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "      'learning_rate': ['adaptive'],\n",
    "      'max_iter': [100, 200, 300]\n",
    "  }\n",
    "\n",
    "  random_forest = RandomForestClassifier(n_jobs = -1, random_state = RANDOM_STATE)\n",
    "  mlp = MLPClassifier(random_state = RANDOM_STATE)\n",
    "\n",
    "  grid_forest_clf = RandomizedSearchCV(random_forest, param_grid_tree, cv=kfold, scoring=\"accuracy\", n_jobs= -1)\n",
    "  grid_forest_clf.fit(X,y)\n",
    "  best_params_forest = grid_forest_clf.best_params_\n",
    "  print(best_params_forest)\n",
    "\n",
    "  grid_mlp_clf = RandomizedSearchCV(mlp, param_grid_MLP, cv=kfold, scoring=\"accuracy\", n_jobs= -1)\n",
    "  grid_mlp_clf.fit(X,y)\n",
    "  best_params_mlp = grid_mlp_clf.best_params_\n",
    "  print(best_params_mlp)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "DQjw0Rcpjo8T",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00xAQ0HfXMmZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "\n",
    "\"\"\"\n",
    "The optimal hyperparameters for each model found by the random search in the above cell. \n",
    "If you run the random search tuning, comment out the 2 dictionaries below.\n",
    "\"\"\"\n",
    "kfold = KFold(n_splits=10, random_state=RANDOM_STATE, shuffle=True) #K-folding\n",
    "\n",
    "# Optimal parameters for the random forest \n",
    "best_params_forest = {'n_estimators': 100, \n",
    "                      'min_samples_leaf': 2, \n",
    "                      'max_leaf_nodes': 18, \n",
    "                      'max_features': 'auto', \n",
    "                      'max_depth': 15, \n",
    "                      'criterion': 'gini',\n",
    "                      'bootstrap': False}\n",
    "                      \n",
    "# Optimal parameters for the MLP\n",
    "best_params_mlp = {'solver': 'adam', \n",
    "                   'max_iter': 300, \n",
    "                   'learning_rate': 'adaptive', \n",
    "                   'activation': 'relu'\n",
    "                   }\n",
    "\n",
    "cls1 = RandomForestClassifier(**best_params_forest, n_jobs = -1, random_state = RANDOM_STATE)\n",
    "cls2 = MLPClassifier(**best_params_mlp, random_state = RANDOM_STATE)\n",
    "\n",
    "model = [\n",
    "         ('Forest_Tuned', cls1), \n",
    "         ('MLP_Tuned', cls2)]\n",
    "         \n",
    "best_cls = StackingClassifier(estimators = model, final_estimator = LogisticRegression(), n_jobs=-1)\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(best_cls, X, y, scoring=scoring, cv=kfold, return_train_score = True, n_jobs=1, error_score='raise')\n",
    "\n",
    "best_fmeasure = np.max(scores['test_f1'])\n",
    "best_accuracy = np.max(scores['test_accuracy'])\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbLB09agXMma",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "435617e2-5ed6-4c8d-8ad6-7fcada69d451",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifier:\n",
      "F1 Weighted-Score:0.9094412331406552 & Balanced Accuracy:0.888095238095238\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "#print(best_cls)\n",
    "print(\"F1 Weighted-Score:{} & Balanced Accuracy:{}\".format(best_fmeasure, best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnos1uqzXMma",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**3.2** Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5dAfbTfXMmb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "# ANSWER \n",
    "The idea here is to make a simple, yet strong ensemble to perform well on the problem defined by a dataset. In order to achieve that we choose the following 2 classifiers:\n",
    "* **Multilayer Perceptron Neural Network (MLP)**\n",
    " \n",
    " MLPs can perform really well on:\n",
    "    * Tabular datasets\n",
    "    * Classification prediction problems\n",
    "    * Regression prediction problems\n",
    "    * More complex problems\n",
    "  \n",
    "  and we are dealing with a classification/prediction problem, although training can get slow\n",
    "\n",
    "* **Random Forests**\n",
    "  \n",
    "    A Random Forest is a very powerful model for the following reasons:\n",
    "  \n",
    "    * Random Forest can be used to solve both classification as well as regression problems.\n",
    "\n",
    "    * Random Forest works well with both categorical and continuous variables and can automatically handle missing values. Random Forest is also strong with outlier handling\n",
    "\n",
    "    * No feature scaling required: No feature scaling (standardization and normalization) required in case of Random Forest as it uses rule based approach instead of distance calculation.\n",
    "\n",
    "    * Handles non-linear parameters efficiently: Non linear parameters don't affect the performance of a Random Forest unlike curve based algorithms. So, if there is high non-linearity between the independent variables, Random Forest may outperform as compared to other curve based algorithms.\n",
    "\n",
    "    *  Random Forest algorithm is very stable. Even if a new data point is introduced in the dataset, the overall algorithm is not affected much since the new data may impact one tree, but it is very hard for it to impact all the trees.\n",
    "\n",
    "The general idea here is to reduce variance with the random forest classifier and avoid bias problems with the MLP.\n",
    "\n",
    "Now that we have selected our 2 models for our problem, we shall tune the hyperparameters of every model. For complexity reasons, it is better to tune each classifier individually. In the first cell of ***3.0*** a indicative example of a random grid search. Random search reduces search time significantly and still returns adequate results. \n",
    "\n",
    "Using these 2 already powerful models and fine-tuning them, we can then stack them to combine them. The result is well above 85% accuracy and F1 score, which means this model should perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQEFCmbcXMmb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**3.3** Create a classifier that is going to be used in production - in a live system. Use the *test_set_noclass.csv* to make predictions. Store the predictions in a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQPgm_ubXMmc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "cls = best_cls\n",
    "cls.fit(X,y)\n",
    "#END CODE HERE\n",
    "test_set = pd.read_csv(\"test_set_noclass.csv\")\n",
    "predictions = cls.predict(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnAp-d2DXMmf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "LEAVE HERE ANY COMMENTS ABOUT YOUR CLASSIFIER\n",
    "\n",
    "We could have used the classifier from question ***1.2 - Stacking*** but that model had high complexity. We managed to achieve same results without making the model too complex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Neagvu0TXMmg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### This following cell will not be executed. The test_set.csv with the classes will be made available after the deadline and this cell is for testing purposes!!! Do not modify it! ###"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "JF-nPWaJouBM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7K7iI7BXMmg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "  final_test_set = pd.read_csv('test_set.csv')\n",
    "  ground_truth = final_test_set['CLASS']\n",
    "  print(\"Balanced Accuracy: {}\".format(balanced_accuracy_score(predictions, ground_truth)))\n",
    "  print(\"F1 Weighted-Score: {}\".format(f1_score(predictions, ground_truth, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJH-9KdOzW7z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Both should aim above 85%!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EnsembleMethods.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
